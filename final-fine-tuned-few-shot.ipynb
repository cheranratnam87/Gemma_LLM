{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7716369,"sourceType":"datasetVersion","datasetId":4506556},{"sourceId":7716760,"sourceType":"datasetVersion","datasetId":4506826},{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cheranratnam/final-fine-tuned-few-shot?scriptVersionId=164628138\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Competition Overview\n\n**The goal of this competition is to create notebooks that demonstrate how to use the Gemma LLM to accomplish one or more of the following developer-oriented tasks**:\n- Answer common questions about the Kaggle platform. \n\nI have focused on this task:\n**- Explain or teach basic data science concepts.**\n\n\n- Summarize Kaggle Solution write ups.\n- Explain or teach concepts from Kaggle Solution write ups.\n- Answer common questions about the Python programming language.\n\n\nSubmissions to this competition take the form of Kaggle Notebooks. Your notebook should demonstrate how to use the Gemma model to complete the task that you have selected. \n\n**How can Gemma be used to assist Kaggle developers? Show us your ideas today!**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-28T03:26:04.784648Z","iopub.execute_input":"2024-02-28T03:26:04.785074Z","iopub.status.idle":"2024-02-28T03:26:05.958325Z","shell.execute_reply.started":"2024-02-28T03:26:04.78504Z","shell.execute_reply":"2024-02-28T03:26:05.957243Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n/kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\n/kaggle/input/gemma/transformers/2b/2/gemma-2b.gguf\n/kaggle/input/gemma/transformers/2b/2/config.json\n/kaggle/input/gemma/transformers/2b/2/model-00001-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/model-00002-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/tokenizer.json\n/kaggle/input/gemma/transformers/2b/2/tokenizer_config.json\n/kaggle/input/gemma/transformers/2b/2/special_tokens_map.json\n/kaggle/input/gemma/transformers/2b/2/.gitattributes\n/kaggle/input/gemma/transformers/2b/2/tokenizer.model\n/kaggle/input/gemma/transformers/2b/2/generation_config.json\n/kaggle/input/gemma/transformers/2b-it/1/model.safetensors.index.json\n/kaggle/input/gemma/transformers/2b-it/1/gemma-2b-it.gguf\n/kaggle/input/gemma/transformers/2b-it/1/config.json\n/kaggle/input/gemma/transformers/2b-it/1/model-00001-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b-it/1/model-00002-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b-it/1/tokenizer.json\n/kaggle/input/gemma/transformers/2b-it/1/tokenizer_config.json\n/kaggle/input/gemma/transformers/2b-it/1/special_tokens_map.json\n/kaggle/input/gemma/transformers/2b-it/1/.gitattributes\n/kaggle/input/gemma/transformers/2b-it/1/tokenizer.model\n/kaggle/input/gemma/transformers/2b-it/1/generation_config.json\n/kaggle/input/data-science-concepts-data-set-public-version/final_ds_data.csv\n/kaggle/input/ds-data-concepts/final_ds_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Use PIP to install or upgrade Hugging Face transformers library from GitHub\n!pip install -U git+https://github.com/huggingface/transformers  accelerate bitsandbytes \n\n#installing other packages as needed\n\n!pip install transformers\n!pip install bitsandbytes accelerate\n!pip install qdrant_client\n!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:26:05.960058Z","iopub.execute_input":"2024-02-28T03:26:05.960476Z","iopub.status.idle":"2024-02-28T03:28:14.477528Z","shell.execute_reply.started":"2024-02-28T03:26:05.960448Z","shell.execute_reply":"2024-02-28T03:28:14.476064Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-kxnnt72h\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-kxnnt72h\n  Resolved https://github.com/huggingface/transformers to commit bd5b9863060c31f60d66b6aec88b9743d3dcd8f4\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.0.dev0) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8593740 sha256=79bc5df6d1d0cb62dd9abdb0816cf6fe0470316e43bd34c7aaf43c5d9a148974\n  Stored in directory: /tmp/pip-ephem-wheel-cache-97gx6pdm/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: bitsandbytes, transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\nSuccessfully installed bitsandbytes-0.42.0 transformers-4.39.0.dev0\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.0.dev0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.42.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting qdrant_client\n  Downloading qdrant_client-1.7.3-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: grpcio>=1.41.0 in /opt/conda/lib/python3.10/site-packages (from qdrant_client) (1.51.1)\nCollecting grpcio-tools>=1.41.0 (from qdrant_client)\n  Downloading grpcio_tools-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nRequirement already satisfied: httpx>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (0.27.0)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from qdrant_client) (1.26.4)\nCollecting portalocker<3.0.0,>=2.7.0 (from qdrant_client)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: pydantic>=1.10.8 in /opt/conda/lib/python3.10/site-packages (from qdrant_client) (2.5.3)\nRequirement already satisfied: urllib3<3,>=1.26.14 in /opt/conda/lib/python3.10/site-packages (from qdrant_client) (1.26.18)\nCollecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools>=1.41.0->qdrant_client)\n  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting grpcio>=1.41.0 (from qdrant_client)\n  Downloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (69.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.0.4)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (0.14.0)\nCollecting h2<5,>=3 (from httpx[http2]>=0.14.0->qdrant_client)\n  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (4.9.0)\nCollecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client)\n  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\nCollecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client)\n  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.2.0)\nDownloading qdrant_client-1.7.3-py3-none-any.whl (206 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.3/206.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading grpcio_tools-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\nDownloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\nInstalling collected packages: protobuf, portalocker, hyperframe, hpack, grpcio, h2, grpcio-tools, qdrant_client\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.3 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.3 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.3 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.3 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.3 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.3 which is incompatible.\ntensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.3 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.3 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed grpcio-1.60.0 grpcio-tools-1.62.0 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 portalocker-2.8.2 protobuf-4.21.12 qdrant_client-1.7.3\nCollecting sentence_transformers\n  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.39.0.dev0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#These imports allow using AutoTokenizer to tokenize text, load pre-trained models for causal modeling\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nfrom qdrant_client import models, QdrantClient\nfrom sentence_transformers import SentenceTransformer\nfrom fuzzywuzzy import fuzz","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:28:14.480047Z","iopub.execute_input":"2024-02-28T03:28:14.480477Z","iopub.status.idle":"2024-02-28T03:28:24.857059Z","shell.execute_reply.started":"2024-02-28T03:28:14.480437Z","shell.execute_reply":"2024-02-28T03:28:24.855852Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Quick Note on the imports above**\n\nHugging Face Transformers (transformers.AutoTokenizer and transformers.AutoModelForCausalLM):\n\nDescription: These components facilitate the use of pre-trained language models for natural language processing tasks. AutoTokenizer loads tokenizers, while AutoModelForCausalLM loads models for tasks like text generation by predicting the next word in a sequence.\nBitsAndBytesConfig:\n\nDescription: A configuration class in the Hugging Face Transformers library, BitsAndBytesConfig is specifically tailored for configuring models in tasks related to bits and bytes processing.\nPyTorch (torch module):\n\nDescription: PyTorch's torch module is a fundamental part of the PyTorch library, providing tools for building and training deep neural networks. It is widely used in various machine learning applications, including natural language processing and computer vision.\nQdrant (qdrant_client.models and QdrantClient):\n\nDescription: Components of the Qdrant library, qdrant_client.models includes structures for Qdrant operations, and QdrantClient is a client for efficient interaction with Qdrant. Qdrant is designed for storing and retrieving vector embeddings, commonly used in similarity search and recommendation systems.\nSentenceTransformer:\n\nDescription: SentenceTransformer is a Python library designed for creating sentence embeddings using pre-trained transformer models. These embeddings are valuable for tasks such as measuring semantic similarity between sentences and clustering.\nFuzzywuzzy (fuzzywuzzy.fuzz):\n\nDescription: The fuzz.ratio function from the fuzzywuzzy library is used for fuzzy string matching. It calculates the similarity ratio between two strings, making it useful for tasks where approximate string matching or similarity scores are needed, such as finding similar text.\n","metadata":{}},{"cell_type":"code","source":"#Loading the model in 4bit - This is so we can use our processing power more efficiently\nquantization_config = BitsAndBytesConfig(load_in_8bit=False) ","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:28:24.858394Z","iopub.execute_input":"2024-02-28T03:28:24.859206Z","iopub.status.idle":"2024-02-28T03:28:24.864201Z","shell.execute_reply.started":"2024-02-28T03:28:24.859174Z","shell.execute_reply":"2024-02-28T03:28:24.863178Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Define model and data type - We will use this model throughout our approach\nmodel_id = \"/kaggle/input/gemma/transformers/2b-it/1\" \ndtype = torch.float16","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:28:24.867279Z","iopub.execute_input":"2024-02-28T03:28:24.868491Z","iopub.status.idle":"2024-02-28T03:28:24.88475Z","shell.execute_reply.started":"2024-02-28T03:28:24.868447Z","shell.execute_reply":"2024-02-28T03:28:24.883609Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Set up a language model and tokenizer using a pre-trained model with automatic device mapping, data type, and configurations\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=dtype,\n    quantization_config = quantization_config\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:28:24.886804Z","iopub.execute_input":"2024-02-28T03:28:24.887251Z","iopub.status.idle":"2024-02-28T03:29:02.722938Z","shell.execute_reply.started":"2024-02-28T03:28:24.887212Z","shell.execute_reply":"2024-02-28T03:29:02.722019Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c015ff0ed02458b923acbda4119dbf2"}},"metadata":{}}]},{"cell_type":"code","source":"# Test the base model - Note the output the base model generates to compare after we make some fine tuning\ninput_text = \"What is data science?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**input_ids, max_new_tokens=250)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:02.7257Z","iopub.execute_input":"2024-02-28T03:29:02.726473Z","iopub.status.idle":"2024-02-28T03:29:30.25299Z","shell.execute_reply.started":"2024-02-28T03:29:02.72643Z","shell.execute_reply":"2024-02-28T03:29:30.25168Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1482: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n2024-02-28 03:29:05.873827: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-28 03:29:05.873957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-28 03:29:06.039251: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"<bos>What is data science?\n\nData science is the rapidly growing field that combines the power of data analysis, statistical modeling, and computer science to extract meaningful insights from complex and ever-growing datasets. It involves a wide range of tasks, from data wrangling and cleaning to data analysis, modeling, and visualization. Data scientists use their skills to uncover hidden patterns, identify trends, and make informed decisions based on data insights.\n\n**Key components of data science include:**\n\n* **Data wrangling and cleaning:** Gathering, transforming, and organizing data from various sources.\n* **Data analysis and modeling:** Using statistical methods and machine learning algorithms to identify patterns and relationships.\n* **Data visualization:** Creating clear and insightful visualizations to communicate data insights.\n* **Data wrangling and cleaning:** Gathering, transforming, and organizing data from various sources.\n* **Statistical modeling:** Using statistical methods and machine learning algorithms to identify patterns and relationships.\n* **Data visualization:** Creating clear and insightful visualizations to communicate data insights.\n\n**Benefits of data science:**\n\n* **Improved decision-making:** Data-driven insights help organizations make more informed and efficient decisions.\n* **Increased revenue:** By identifying market trends and customer behavior, businesses can increase revenue.\n* **\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Now I will load a custom data set I created to improve results of the base model ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/data-science-concepts-data-set-public-version/final_ds_data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:30.254875Z","iopub.execute_input":"2024-02-28T03:29:30.255935Z","iopub.status.idle":"2024-02-28T03:29:30.286582Z","shell.execute_reply.started":"2024-02-28T03:29:30.255884Z","shell.execute_reply":"2024-02-28T03:29:30.285255Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  Data science is an interdisciplinary academic ...   \n1  Top 3 data science concepts are: 1. Data Types...   \n2  \\nThe term “Data Science” was created in the e...   \n3  Statistics, Visualization, Deep Learning, Mach...   \n4  \\nStatistics is the most critical unit of Data...   \n\n                         label  \n0        what is data science?  \n1  top 3 data science concepts  \n2      history of data science  \n3  basic data science concepts  \n4          what is statistics?  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data science is an interdisciplinary academic ...</td>\n      <td>what is data science?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Top 3 data science concepts are: 1. Data Types...</td>\n      <td>top 3 data science concepts</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\nThe term “Data Science” was created in the e...</td>\n      <td>history of data science</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Statistics, Visualization, Deep Learning, Mach...</td>\n      <td>basic data science concepts</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\nStatistics is the most critical unit of Data...</td>\n      <td>what is statistics?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#just want to process and adjust the text - if needed (I had this issue in a previous version - always good to process)\ndf['text'] = df['text'].apply(lambda x: x.replace(\"\\n\", \"\"))\n\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:30.288105Z","iopub.execute_input":"2024-02-28T03:29:30.288509Z","iopub.status.idle":"2024-02-28T03:29:30.30062Z","shell.execute_reply.started":"2024-02-28T03:29:30.288476Z","shell.execute_reply":"2024-02-28T03:29:30.299097Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"                                                 text  \\\n0   Data science is an interdisciplinary academic ...   \n1   Top 3 data science concepts are: 1. Data Types...   \n2   The term “Data Science” was created in the ear...   \n3   Statistics, Visualization, Deep Learning, Mach...   \n4   Statistics is the most critical unit of Data S...   \n5   Visualization technique helps you access huge ...   \n6   Machine Learning explores the building and stu...   \n7   Deep Learning method is new machine learning r...   \n8   Data Engineering is the process of organizing,...   \n9   A convolutional neural network (CNN) is a cate...   \n10  The pooling layer of a CNN is a critical compo...   \n11  The process starts by sliding a filter designe...   \n12  In 1974, Peter Naur authored the Concise Surve...   \n13  The term \"data science\" has been traced back t...   \n14  Just as the name implies, data science is a br...   \n15  Data wrangling is the process of converting da...   \n16  Data Visualization is one of the most importan...   \n17  An outlier is a data point that is very differ...   \n18  Most datasets contain missing values. The easi...   \n19  Top 5 data science concepts are: data set, dat...   \n\n                                    label  \n0                   what is data science?  \n1             top 3 data science concepts  \n2                 history of data science  \n3             basic data science concepts  \n4                     what is statistics?  \n5                  what is visualization?  \n6               what is machine learning?  \n7                  what is deep learning?  \n8               what is data engineering?  \n9                          What is a CNN?  \n10               What is a pooling layer?  \n11                    How does CNNs work?  \n12             who invented data science?  \n13     Who is the father of Data Science?  \n14                    What is a data set?  \n15                What is data wrangling?  \n16            What is data visualization?  \n17                    What is an outlier?  \n18               What is data imputation?  \n19  what are top 5 data science concepts?  \n","output_type":"stream"}]},{"cell_type":"code","source":"df = df[df['text'].notna()] # remove any NaN values as it blows up serialization (there are no in this dataset but as a good practice)\ndata = df.to_dict('records') # creating a dictionary\nlen(data)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:30.301981Z","iopub.execute_input":"2024-02-28T03:29:30.302408Z","iopub.status.idle":"2024-02-28T03:29:30.322244Z","shell.execute_reply.started":"2024-02-28T03:29:30.302376Z","shell.execute_reply":"2024-02-28T03:29:30.320979Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}]},{"cell_type":"markdown","source":"# Using pre-trained Sentence Transformer model to encode textual data into vectors \n\n**Then searches for similar vectors in an in-memory Qdrant collection. The goal is to retrieve documents that are similar to a given query (user prompt).**","metadata":{}},{"cell_type":"markdown","source":"**Here, a Sentence Transformer model named 'all-MiniLM-L6-v2' is loaded. It's a pre-trained model designed for converting sentences into numerical vectors.**","metadata":{}},{"cell_type":"code","source":"encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:30.32623Z","iopub.execute_input":"2024-02-28T03:29:30.326683Z","iopub.status.idle":"2024-02-28T03:29:33.477465Z","shell.execute_reply.started":"2024-02-28T03:29:30.326648Z","shell.execute_reply":"2024-02-28T03:29:33.476274Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575987bd07f3473a9a0535a124f6c444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bb7d7685224435a9509c53b287770f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"846fe78178af4ab1b71d0d25cb11e0f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b312ccd586bb4285b3c19468929c2548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f7ce6b80974e9c9bec2b425c767550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9cffbd6c6b04216a710be20a14892fb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0499de91f2474bb28c5efdcefd0209f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"058e64c7fa084bd88cc9a789c8dd782c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2017732880794adb8c3c34684110e877"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aae4d99834c459b8c1f870cadcad12e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7208b1375a945ecb35598ea912bdb1a"}},"metadata":{}}]},{"cell_type":"markdown","source":"**A QdrantClient is created, representing an in-memory instance of the Qdrant vector database.**","metadata":{}},{"cell_type":"code","source":"# create the vector database client\nqdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:33.479306Z","iopub.execute_input":"2024-02-28T03:29:33.480178Z","iopub.status.idle":"2024-02-28T03:29:33.486719Z","shell.execute_reply.started":"2024-02-28T03:29:33.480131Z","shell.execute_reply":"2024-02-28T03:29:33.485318Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**This code initializes a Qdrant collection named 'ds_concepts' to store vectorized representations of data. The vectors have a size determined by the Sentence Transformer model, and cosine distance is used for similarity calculations.**","metadata":{}},{"cell_type":"code","source":"# Create collection of data Science Concepts\nqdrant.recreate_collection(\n    collection_name=\"ds_concepts\",\n    vectors_config=models.VectorParams(\n        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n        distance=models.Distance.COSINE\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:33.488385Z","iopub.execute_input":"2024-02-28T03:29:33.489291Z","iopub.status.idle":"2024-02-28T03:29:33.511242Z","shell.execute_reply.started":"2024-02-28T03:29:33.489237Z","shell.execute_reply":"2024-02-28T03:29:33.509877Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"**Data from the 'data' variable, which holds the data science concepts information is vectorized using the Sentence Transformer model and uploaded to the Qdrant collection 'ds_concepts'.**","metadata":{}},{"cell_type":"code","source":"# vectorize!\nqdrant.upload_points(\n    collection_name=\"ds_concepts\",\n    points=[\n        models.PointStruct(\n            id=idx,\n            vector=encoder.encode(doc[\"text\"]).tolist(),\n            payload=doc,\n        ) for idx, doc in enumerate(data)  # data is the variable holding all the data science concepts\n    ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:33.512801Z","iopub.execute_input":"2024-02-28T03:29:33.513275Z","iopub.status.idle":"2024-02-28T03:29:34.329687Z","shell.execute_reply.started":"2024-02-28T03:29:33.513235Z","shell.execute_reply":"2024-02-28T03:29:34.328456Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bfcdb5396a740c991bfaa8a2b27cd4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a62675d4b4e43b1a8f7e7e5b0d459a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c104cabf31942319f33c195f9668f33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551a6512b93b4c13a073a968a7530a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1044979aea4d678c3b2f92e96a3053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afddaf77ac0248e8bd50a78489736c6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a77488f9ede415a8bb5537040223013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c9738f9cb1a43a69283784244c3d8c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1670ec89583b43288d518a7a5ae0a8a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78752110712947a4ad0fcaef2f962520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dacc13dcb444fb8bd10e746f6630e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"470da7502a924a979e65cebb6fdc68d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815090f5e4894783baed2b79891a2211"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44f0dd949c44f5cac3481d6455fe492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd7f050f0b1e40e6af6e6411827b52c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d735461bbea4a738b298201bcbf8665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14004a0512de4fb8a661ae8bbd5975d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d530c84112f9488db98cb23e0376bff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea67c15a79c74e25bcefa8818561fdc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7511837e54f45e6a680d26f2637a9ec"}},"metadata":{}}]},{"cell_type":"code","source":"user_prompt = \"Top 5 Data Science Concepts\"","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:34.331498Z","iopub.execute_input":"2024-02-28T03:29:34.332063Z","iopub.status.idle":"2024-02-28T03:29:34.33805Z","shell.execute_reply.started":"2024-02-28T03:29:34.332013Z","shell.execute_reply":"2024-02-28T03:29:34.336744Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**A search is performed in the 'ds_concepts' collection to find similar vectors to the one generated from the user prompt \"Top 5 Data Science Concepts\". You can limit as needed, I selected 122 randomly.**","metadata":{}},{"cell_type":"code","source":"# Search time for data science concepts!\n\nhits = qdrant.search(\n    collection_name=\"ds_concepts\",\n    query_vector=encoder.encode(user_prompt).tolist(),\n    limit=122\n)\nfor hit in hits:\n  print(hit.payload, \"score:\", hit.score)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:34.33984Z","iopub.execute_input":"2024-02-28T03:29:34.34081Z","iopub.status.idle":"2024-02-28T03:29:34.404971Z","shell.execute_reply.started":"2024-02-28T03:29:34.340732Z","shell.execute_reply":"2024-02-28T03:29:34.403813Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"987e6928e7be4cd0bdfa2aa7b0c26f7b"}},"metadata":{}},{"name":"stdout","text":"{'text': 'Top 5 data science concepts are: data set, data wrangling, data visualization, outlier, and data imputation', 'label': 'what are top 5 data science concepts?'} score: 0.8214098510272281\n{'text': 'Statistics, Visualization, Deep Learning, Machine Learning are important Data Science concepts. Data Science Process goes through Discovery, Data Preparation, Model Planning, Model Building, Operationalize, Communicate Results.', 'label': 'basic data science concepts'} score: 0.5867200099924217\n{'text': 'Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.  However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge', 'label': 'what is data science?'} score: 0.5550271464165655\n{'text': 'In 1974, Peter Naur authored the Concise Survey of Computer Methods, using the term “Data Science,” repeatedly. Naur presented his own convoluted definition of the new concept: “The usefulness of data and data processes derives from their application in building and handling models of reality.”', 'label': 'who invented data science?'} score: 0.5314519069351389\n{'text': 'The term “Data Science” was created in the early 1960s to describe a new profession that would support the understanding and interpretation of the large amounts of data which was being amassed at the time. (At the time, there was no way of predicting the truly massive amounts of data over the next fifty years.) Data Science continues to evolve as a discipline using computer science and statistical methodology to make useful predictions and gain insights in a wide range of fields. While Data Science is used in areas such as astronomy and medicine, it is also used in business to help make smarter decisions.Statistics, and the use of statistical models, are deeply rooted within the field of Data Science. Data Science started with statistics and has evolved to include concepts/practices such as artificial intelligence, machine learning, and the Internet of Things, to name a few. As more and more data has become available, first by way of recorded shopping behaviors and trends, businesses have been collecting and storing it in ever greater amounts. With the growth of the Internet, the Internet of Things, and the exponential growth of data volumes available to enterprises, there has been a flood of new information or big data. Once the doors were opened by businesses seeking to increase profits and drive better decision-making, the use of big data started being applied to other fields, such as medicine, engineering, and social sciences.', 'label': 'history of data science'} score: 0.5163190014523478\n{'text': 'Top 3 data science concepts are: 1. Data TypesImagine a toy store with different types of toys — dolls, action figures, puzzles, and board games. Similarly, data can be numbers, text, images, and more. Understanding these types is crucial for handling data effectively.Example: Age (numeric), Name (text), Image (image data).2. VariablesVariables are containers that hold data. Think of them as labeled jars storing candies. The labels are like names for the jars, and the candies are your data.Example: Age can be a variable storing numbers representing different ages.3. Descriptive StatisticsDescriptive statistics are like summarizing a long book into a few sentences. It helps us understand the main points of a dataset.', 'label': 'top 3 data science concepts'} score: 0.49688939732123855\n{'text': 'Just as the name implies, data science is a branch of science that applies the scientific method to data with the goal of studying the relationships between different features and drawing out meaningful conclusions based on these relationships. Data is, therefore, the key component in data science. A dataset is a particular instance of data that is used for analysis or model building at any given time. A dataset comes in different flavors such as numerical data, categorical data, text data, image data, voice data, and video data. A dataset could be static (not changing) or dynamic (changes with time, for example, stock prices). Moreover, a dataset could depend on space as well. For example, temperature data in the United States would differ significantly from temperature data in Africa. For beginning data science projects, the most popular type of dataset is a dataset containing numerical data that is typically stored in a comma-separated values (CSV) file format.', 'label': 'What is a data set?'} score: 0.43830041039850043\n{'text': 'Data Visualization is one of the most important branches of data science. It is one of the main tools used to analyze and study relationships between different variables. Data visualization (e.g., scatter plots, line graphs, bar plots, histograms, qqplots, smooth densities, boxplots, pair plots, heat maps, etc.) can be used for descriptive analytics. Data visualization is also used in machine learning for data preprocessing and analysis, feature selection, model building, model testing, and model evaluation. When preparing a data visualization, keep in mind that data visualization is more of an Art than Science.', 'label': 'What is data visualization?'} score: 0.4326190007379821\n{'text': 'Statistics is the most critical unit of Data Science basics, and it is the method or science of collecting and analyzing numerical data in large quantities to get useful insights.', 'label': 'what is statistics?'} score: 0.426284271850401\n{'text': 'Data wrangling is the process of converting data from its raw form to a tidy form ready for analysis. Data wrangling is an important step in data preprocessing and includes several processes like data importing, data cleaning, data structuring, string processing, HTML parsing, handling dates and times, handling missing data, and text mining.', 'label': 'What is data wrangling?'} score: 0.38574345191998505\n{'text': \"Data Engineering is the process of organizing, managing, and analyzing large amounts of data. It's a key component in the world of data science, but it can be used by anyone who has to deal with big data regularly. Data engineering is about collecting, storing, and processing data.\", 'label': 'what is data engineering?'} score: 0.38241867045690653\n{'text': 'The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science', 'label': 'Who is the father of Data Science?'} score: 0.3790114149664714\n{'text': 'Machine Learning explores the building and study of algorithms that learn to make predictions about unforeseen/future data.', 'label': 'what is machine learning?'} score: 0.37342543119957505\n{'text': 'Visualization technique helps you access huge amounts of data in easy to understand and digestible visuals.', 'label': 'what is visualization?'} score: 0.3637461020695629\n{'text': 'Deep Learning method is new machine learning research where the algorithm selects the analysis model to follow.', 'label': 'what is deep learning?'} score: 0.2383612562448082\n{'text': 'An outlier is a data point that is very different from the rest of the dataset. Outliers are often just bad data, e.g., due to a malfunctioned sensor; contaminated experiments; or human error in recording data. Sometimes, outliers could indicate something real such as a malfunction in a system. Outliers are very common and are expected in large datasets. One common way to detect outliers in a dataset is by using a box plot. Figure 3 shows a simple regression model for a dataset containing lots of outliers. Outliers can significantly degrade the predictive power of a machine learning model. A common way to deal with outliers is to simply omit the data points. However, removing real data outliers can be too optimistic, leading to non-realistic models. Advanced methods for dealing with outliers include the RANSAC method.', 'label': 'What is an outlier?'} score: 0.1728470811270934\n{'text': 'A convolutional neural network (CNN) is a category of machine learning model, namely a type of deep learning algorithm well suited to analyzing visual data. CNNs -- sometimes referred to as convnets -- use principles from linear algebra, particularly convolution operations, to extract features and identify patterns within images. Although CNNs are predominantly used to process images, they can also be adapted to work with audio and other signal data. CNNs use a series of layers, each of which detects different features of an input image. Depending on the complexity of its intended purpose, a CNN can contain dozens, hundreds or even thousands of layers, each building on the outputs of previous layers to recognize detailed patterns.', 'label': 'What is a CNN?'} score: 0.1490649715360372\n{'text': 'The process starts by sliding a filter designed to detect certain features over the input image, a process known as the convolution operation (hence the name \"convolutional neural network\"). The result of this process is a feature map that highlights the presence of the detected features in the image. This feature map then serves as input for the next layer, enabling a CNN to gradually build a hierarchical representation of the image. Initial filters usually detect basic features, such as lines or simple textures. Subsequent layers\\' filters are more complex, combining the basic features identified earlier on to recognize more complex patterns. For example, after an initial layer detects the presence of edges, a deeper layer could use that information to start identifying shapes. Between these layers, the network takes steps to reduce the spatial dimensions of the feature maps to improve efficiency and accuracy. In the final layers of a CNN, the model makes a final decision -- for example, classifying an object in an image -- based on the output from the previous layers.', 'label': 'How does CNNs work?'} score: 0.1310934798491832\n{'text': 'Most datasets contain missing values. The easiest way to deal with missing data is simply to throw away the data point. However, the removal of samples or dropping of entire feature columns is simply not feasible because we might lose too much valuable data. In this case, we can use different interpolation techniques to estimate the missing values from the other training samples in our dataset. One of the most common interpolation techniques is mean imputation, where we simply replace the missing value with the mean value of the entire feature column. Other options for imputing missing values are median or most frequent (mode), where the latter replaces the missing values with the most frequent values. Whatever imputation method you employ in your model, you have to keep in mind that imputation is only an approximation, and hence can produce an error in the final model. If the data supplied was already preprocessed, you would have to find out how missing values were considered. What percentage of the original data was discarded? What imputation method was used to estimate missing values?', 'label': 'What is data imputation?'} score: 0.12967782005655537\n{'text': \"The pooling layer of a CNN is a critical component that follows the convolutional layer. Similar to the convolutional layer, the pooling layer's operations involve a sweeping process across the input image, but its function is otherwise different. The pooling layer aims to reduce the dimensionality of the input data while retaining critical information, thus improving the network's overall efficiency. This is typically achieved through downsampling: decreasing the number of data points in the input. For CNNs, this typically means reducing the number of pixels used to represent the image. The most common form of pooling is max pooling, which retains the maximum value within a certain window (i.e., the kernel size) while discarding other values. Another common technique, known as average pooling, takes a similar approach but uses the average value instead of the maximum.\", 'label': 'What is a pooling layer?'} score: 0.07605453693948906\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**The search results are printed, including the payload (original document information) and the similarity score.**","metadata":{}},{"cell_type":"code","source":"# define a variable to hold the search results\nsearch_results = [hit.payload for hit in hits]","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:34.4068Z","iopub.execute_input":"2024-02-28T03:29:34.407659Z","iopub.status.idle":"2024-02-28T03:29:34.413207Z","shell.execute_reply.started":"2024-02-28T03:29:34.407608Z","shell.execute_reply":"2024-02-28T03:29:34.411924Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"for hit in search_results[:5]:\n    print(hit)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:29:34.414874Z","iopub.execute_input":"2024-02-28T03:29:34.415851Z","iopub.status.idle":"2024-02-28T03:29:34.427398Z","shell.execute_reply.started":"2024-02-28T03:29:34.415805Z","shell.execute_reply":"2024-02-28T03:29:34.426144Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"{'text': 'Top 5 data science concepts are: data set, data wrangling, data visualization, outlier, and data imputation', 'label': 'what are top 5 data science concepts?'}\n{'text': 'Statistics, Visualization, Deep Learning, Machine Learning are important Data Science concepts. Data Science Process goes through Discovery, Data Preparation, Model Planning, Model Building, Operationalize, Communicate Results.', 'label': 'basic data science concepts'}\n{'text': 'Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.  However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge', 'label': 'what is data science?'}\n{'text': 'In 1974, Peter Naur authored the Concise Survey of Computer Methods, using the term “Data Science,” repeatedly. Naur presented his own convoluted definition of the new concept: “The usefulness of data and data processes derives from their application in building and handling models of reality.”', 'label': 'who invented data science?'}\n{'text': 'The term “Data Science” was created in the early 1960s to describe a new profession that would support the understanding and interpretation of the large amounts of data which was being amassed at the time. (At the time, there was no way of predicting the truly massive amounts of data over the next fifty years.) Data Science continues to evolve as a discipline using computer science and statistical methodology to make useful predictions and gain insights in a wide range of fields. While Data Science is used in areas such as astronomy and medicine, it is also used in business to help make smarter decisions.Statistics, and the use of statistical models, are deeply rooted within the field of Data Science. Data Science started with statistics and has evolved to include concepts/practices such as artificial intelligence, machine learning, and the Internet of Things, to name a few. As more and more data has become available, first by way of recorded shopping behaviors and trends, businesses have been collecting and storing it in ever greater amounts. With the growth of the Internet, the Internet of Things, and the exponential growth of data volumes available to enterprises, there has been a flood of new information or big data. Once the doors were opened by businesses seeking to increase profits and drive better decision-making, the use of big data started being applied to other fields, such as medicine, engineering, and social sciences.', 'label': 'history of data science'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Next, We will generate a response from an AI assistant based on user input. The generate_response function takes the user input, search results (retrieved from the Qdrant search), and other model-related parameters to decide whether to provide an answer directly from the retrieved results or generate a new response using a language model.**","metadata":{}},{"cell_type":"markdown","source":"# Key code elements to note in the below approach\n\n* Retrieval-based Response:The code uses the fuzz.ratio function from the fuzzywuzzy library to find the best match in the search results based on the similarity between the user input and the 'label' field in each search result. If the similarity is above a certain threshold (here, 90%), it directly selects the best match.\n* Generation-based Response:If no high similarity match is found in the search results, the user input is tokenized and used as input to a language model for response generation. The generated text is then used as the response.\n* Example Usage in Interactive Loop:This sets up an interactive loop where the user can input queries, and the AI assistant responds based on either the retrieved results or generated content from the language model.\n\n**In summary, this code combines retrieval and generation techniques to provide responses. If a highly similar result is found in the search results, it directly uses that information. Otherwise, it generates a response using a language model, taking into account the user input and the retrieved search results.\n**","metadata":{}},{"cell_type":"markdown","source":"# Using the dataset I uploaded and the base model to provide more comprehensive answers \n\n* Q1. What is data? if the fuzzy ratio was at 80% this would bring an answer from my dataset - What is data science\n* Q2. What is data science? This answer is directly from my data set. You can see what the base model provided for this vs what this model is providing\n* Q3. Give me some data science concepts ... I have several questions about concepts but since I have my ratio at 90% it pulls from the base model\n* Q4. what are top data science concepts? This is one of the answers from my dataset :) Please note I am doing this just to make it clear how the model retrieves specific information and these are just for the sake of testing ... \n* Q5. Who is the father of Data science? Again, just for Kicks :) \n* Q6. Who invented data science?\n\n\n# Now we will go into more data science concepts\n\n* Q7. What is a CNN? It selects from my dataset\n* Q8. What are CNNs used for? This answer comes from the base model\n* Q9. What is a pooling layer? I added this answer to see how the model goes back and forth :)\n* Q10. What is data engineering? This picks from my dataset as well. \n\n##It is important to note that your fuzzy ratio comes into play when it comes to when the model will choose your answer vs the base models'.\n\nIn a business scenario, one could have a lower fuzzy ratio to perhaps retrieve organizational specific information over more generic information retrieved from the base model.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"from fuzzywuzzy import fuzz\n\ndef generate_response(user_input, search_results, tokenizer, model, max_length=250):\n    # Find the best matching 'label_column'\n    best_match = max(search_results, key=lambda hit: fuzz.ratio(user_input.lower(), hit['label'].lower()))\n\n    # Check if the best match has a high enough similarity\n    if fuzz.ratio(user_input.lower(), best_match['label'].lower()) >= 90:\n        answer = f\"{best_match['text']} (related concept: {best_match['label']}).\"\n    else:\n        # Tokenize the user input for model-generated response\n        input_ids = tokenizer.encode(user_input, return_tensors=\"pt\", max_length=max_length, truncation=True)\n        input_ids = input_ids.to(model.device)  # Ensure input_ids are on the same device as the model\n    \n        # Generate response with max_new_tokens\n        outputs = model.generate(input_ids, max_length=max_length, max_new_tokens=200, num_return_sequences=1)\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        answer = generated_text\n\n    return answer\n\n# Example usage in an interactive loop\nwhile True:\n    user_input = input(\"You: \")\n    \n    # Exit the loop if the user types 'exit'\n    if user_input.lower() == 'exit':\n        print(\"AI Assistant: Goodbye! Have a great day.\")\n        break\n\n    # Use the AI assistant to generate a response\n    response = generate_response(user_input, search_results, tokenizer, model)\n    \n    # Print the assistant's response without including search_results\n    print(\"AI Assistant:\", response)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:30:24.049329Z","iopub.execute_input":"2024-02-28T03:30:24.049766Z","iopub.status.idle":"2024-02-28T03:32:15.584208Z","shell.execute_reply.started":"2024-02-28T03:30:24.049733Z","shell.execute_reply":"2024-02-28T03:32:15.583014Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdin","text":"You:  What is data?\n"},{"name":"stderr","text":"Both `max_new_tokens` (=200) and `max_length`(=250) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"AI Assistant: What is data?\n\nData is a collection of facts, figures, and other information that is used to make decisions. It can be numerical, textual, or graphical. Data is often used to track trends, identify patterns, and make predictions.\n\nHere are some examples of data:\n\n* Sales figures\n* Customer names\n* Employee salaries\n* Weather conditions\n* Market trends\n\nWhat is the difference between data and information?\n\nWhile data and information are related, they are not the same thing. Data is a collection of facts, figures, and other information, while information is a synthesis of data that is organized and meaningful.\n\nHere is a simple analogy:\n\n* Data is like the raw materials that are used to build a house.\n* Information is like the finished product that is built from the raw materials.\n\nData is a fundamental part of any decision-making process. By understanding data, you can make more informed decisions and achieve your goals more effectively.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What is data science?\n"},{"name":"stdout","text":"AI Assistant: Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.  However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge (related concept: what is data science?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Give me some data science concepts\n"},{"name":"stderr","text":"Both `max_new_tokens` (=200) and `max_length`(=250) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"AI Assistant: Give me some data science concepts and techniques that I can use to analyze and visualize data in a healthcare setting?\n\n**Data Science Concepts and Techniques for Healthcare Data Analysis and Visualization**\n\n**1. Data wrangling and cleaning:**\n- Data integration from various sources (e.g., EHRs, medical devices, lab reports)\n- Handling missing values and outliers\n- Data transformation and normalization\n\n**2. Data exploration and analysis:**\n- Descriptive statistics (e.g., mean, median, standard deviation)\n- Inferential statistics (e.g., hypothesis testing, regression analysis)\n- Visualization techniques (e.g., scatter plots, boxplots, heatmaps)\n\n**3. Data mining and pattern discovery:**\n- Identifying patterns and relationships in data\n- Clustering, classification, and regression algorithms\n- Association rule mining\n\n**4. Data visualization:**\n- Creating clear and informative visualizations that communicate insights\n- Choosing appropriate visualization techniques (e.g., bar charts, scatter plots, heat\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  what are top data science concepts?\n"},{"name":"stdout","text":"AI Assistant: Top 5 data science concepts are: data set, data wrangling, data visualization, outlier, and data imputation (related concept: what are top 5 data science concepts?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Who is the father of Data science?\n"},{"name":"stdout","text":"AI Assistant: The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science (related concept: Who is the father of Data Science?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Who invented data science?\n"},{"name":"stdout","text":"AI Assistant: In 1974, Peter Naur authored the Concise Survey of Computer Methods, using the term “Data Science,” repeatedly. Naur presented his own convoluted definition of the new concept: “The usefulness of data and data processes derives from their application in building and handling models of reality.” (related concept: who invented data science?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What is a CNN?\n"},{"name":"stdout","text":"AI Assistant: A convolutional neural network (CNN) is a category of machine learning model, namely a type of deep learning algorithm well suited to analyzing visual data. CNNs -- sometimes referred to as convnets -- use principles from linear algebra, particularly convolution operations, to extract features and identify patterns within images. Although CNNs are predominantly used to process images, they can also be adapted to work with audio and other signal data. CNNs use a series of layers, each of which detects different features of an input image. Depending on the complexity of its intended purpose, a CNN can contain dozens, hundreds or even thousands of layers, each building on the outputs of previous layers to recognize detailed patterns. (related concept: What is a CNN?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What are CNNs used for?\n"},{"name":"stderr","text":"Both `max_new_tokens` (=200) and `max_length`(=250) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"AI Assistant: What are CNNs used for?\n\nConvolutional neural networks (CNNs) are a type of artificial neural network (ANN) used for image recognition. They are widely used in various applications, including:\n\n* **Medical diagnosis:** CNNs can be used to analyze medical images, such as X-rays, CT scans, and MRI scans, to detect diseases and abnormalities.\n* **Security and surveillance:** CNNs can be used to identify and track objects or individuals in security footage, detect suspicious activities, and monitor security systems.\n* **Object recognition:** CNNs can be used to identify and classify objects in images, such as faces, animals, and objects in manufacturing.\n* **Natural language processing (NLP):** CNNs can be used to process and understand natural language text and images, such as sentiment analysis, text classification, and image captioning.\n* **Advertising and marketing:** CNNs can be used to analyze customer data and preferences to create targeted advertising campaigns.\n\nCNNs are highly effective at\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What is a pooling layer?\n"},{"name":"stdout","text":"AI Assistant: The pooling layer of a CNN is a critical component that follows the convolutional layer. Similar to the convolutional layer, the pooling layer's operations involve a sweeping process across the input image, but its function is otherwise different. The pooling layer aims to reduce the dimensionality of the input data while retaining critical information, thus improving the network's overall efficiency. This is typically achieved through downsampling: decreasing the number of data points in the input. For CNNs, this typically means reducing the number of pixels used to represent the image. The most common form of pooling is max pooling, which retains the maximum value within a certain window (i.e., the kernel size) while discarding other values. Another common technique, known as average pooling, takes a similar approach but uses the average value instead of the maximum. (related concept: What is a pooling layer?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What is data engineering? \n"},{"name":"stdout","text":"AI Assistant: Data Engineering is the process of organizing, managing, and analyzing large amounts of data. It's a key component in the world of data science, but it can be used by anyone who has to deal with big data regularly. Data engineering is about collecting, storing, and processing data. (related concept: what is data engineering?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"AI Assistant: Goodbye! Have a great day.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Incorporating few-shot prompts to the above model\n\n* Few-shot prompts are used to provide context or guidance to the language model when generating responses. The idea behind few-shot learning is to train a model on a small set of examples (shots) for a particular task or prompt. In your code, the few-shot prompts are combined with the user's input to form a comprehensive prompt for the model.\n\n**Here is how few shot prompting is used in this code:**\n\n* Defining Few-Shot Prompts:\n\nDefine a list of few-shot prompts (few_shot_prompts) related to data science concepts. These prompts are examples of the kinds of questions or tasks you want the model to be able to handle.\n\n* Combining Prompts with User Input:\n\nWhen a user provides input, the code combines the user's input with the few-shot prompts. It creates a new prompt by joining the few-shot prompts and the user's input together.\n\n* Tokenization and Model Input:\n\nThe combined prompt is then tokenized using the tokenizer provided by the Hugging Face Transformers library. This tokenized prompt is then converted into input tensors suitable for the model.\n\n* Model Response Generation:\n\nThe tokenized prompt is fed into the language model using the \"model.generate function\". The model generates a response based on the combined input prompt.\n\n* Decoding and Displaying Response:\n\nThe generated response is then decoded from token IDs to human-readable text using the tokenizer. This response is then printed as the output of your AI assistant.\n\nBy incorporating few-shot prompts, we are essentially guiding the model to understand the context of the user's input better. The prompts act as examples to influence the model's behavior, allowing it to perform specific tasks or answer questions related to data science concepts. The prompts help fine-tune the model's responses based on the specified prompts, making it more tailored to the desired domain of knowledge.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Let's explore some few shot prompts\n\n\n* Q1. \"Can you explain the concept of regularization in the context of machine learning?\"\n* Q2. What is visualization in data science? \n* Q3. What is data visualization? This fetches my definition \n\nI have show an example of how the model would work with few shot prompting and can still pull an answer from my data set in these questions.","metadata":{}},{"cell_type":"code","source":"from fuzzywuzzy import fuzz\n\ndef generate_response(user_input, search_results, tokenizer, model, few_shot_prompts, max_length=550):\n    # Find the best matching 'label_column'\n    best_match = max(search_results, key=lambda hit: fuzz.ratio(user_input.lower(), hit['label'].lower()))\n\n    # Check if the best match has a high enough similarity\n    if fuzz.ratio(user_input.lower(), best_match['label'].lower()) >= 90:\n        answer = f\"{best_match['text']} (related concept: {best_match['label']}).\"\n    else:\n        # Combine few-shot prompts with user input\n        prompts = \"\\n\".join([f\"{prompt}\\n\" for prompt in few_shot_prompts])\n        full_prompt = f\"{prompts}User: {user_input}\"\n\n        # Tokenize the combined prompt for model-generated response\n        input_ids = tokenizer.encode(full_prompt, return_tensors=\"pt\", max_length=max_length, truncation=True)\n        input_ids = input_ids.to(model.device)  # Ensure input_ids are on the same device as the model\n    \n        # Generate response with max_new_tokens\n        outputs = model.generate(input_ids, max_length=max_length, max_new_tokens=200, num_return_sequences=1)\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        answer = generated_text\n\n    return answer\n\n# Example usage in an interactive loop\nwhile True:\n    user_input = input(\"You: \")\n    \n    # Exit the loop if the user types 'exit'\n    if user_input.lower() == 'exit':\n        print(\"AI Assistant: Goodbye! Have a great day.\")\n        break\n\n    # Three few-shot prompts related to data science concepts\n    few_shot_prompts = [\n        \"Explain the following data science concept:\",\n        \"Describe the significance of the following term in data science:\",\n        \"Provide an example use case for the following data science technique:\"\n    ]\n    \n    response = generate_response(user_input, search_results, tokenizer, model, few_shot_prompts)\n    \n    # Print the assistant's response without including search_results\n    print(\"AI Assistant:\", response)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T03:32:25.416239Z","iopub.execute_input":"2024-02-28T03:32:25.416681Z","iopub.status.idle":"2024-02-28T03:33:07.964868Z","shell.execute_reply.started":"2024-02-28T03:32:25.41665Z","shell.execute_reply":"2024-02-28T03:33:07.963616Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdin","text":"You:  Can you explain the concept of regularization in the context of machine learning?\n"},{"name":"stderr","text":"Both `max_new_tokens` (=200) and `max_length`(=550) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"AI Assistant: Explain the following data science concept:\n\nDescribe the significance of the following term in data science:\n\nProvide an example use case for the following data science technique:\nUser: Can you explain the concept of regularization in the context of machine learning?\n\n**Regularization** is a technique used in machine learning to reduce overfitting and improve the generalization performance of a model. It achieves this by adding a penalty term to the loss function that is proportional to the magnitude of the model's weights. This encourages the model to find a simpler solution that is less likely to overfit to the training data.\n\n**Significance of regularization:**\n\n* Reduces overfitting: Overfitting occurs when a model becomes too closely fit to the training data and fails to generalize well to new, unseen data. Regularization helps to prevent overfitting by forcing the model to find a more general solution.\n* Improves generalization performance: By encouraging the model to find a simpler solution, regularization can lead to a model that performs better on unseen data.\n* Reduces model complexity: Regularization can also help to reduce the model complexity, which can lead to improved interpretability and performance.\n\n**Example use case:**\n\nSuppose we have a dataset of customers' purchase data,\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What is visualization in data science?\n"},{"name":"stderr","text":"Both `max_new_tokens` (=200) and `max_length`(=550) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"AI Assistant: Explain the following data science concept:\n\nDescribe the significance of the following term in data science:\n\nProvide an example use case for the following data science technique:\nUser: What is visualization in data science?\n\n**Answer:**\n\n**Term:** Visualization\n\n**Significance:** Visualization is a powerful tool in data science that allows users to communicate complex data insights and patterns in a clear and compelling manner. It helps to identify trends, outliers, and relationships between different data variables, making it easier for stakeholders to understand and make informed decisions based on data.\n\n**Example Use Case:**\n\nImagine a data scientist analyzing sales data for a retail company. By using visualization techniques, the data scientist can create charts and graphs that illustrate the following insights:\n\n* **Sales trends over time:** The data shows that sales have been steadily increasing in recent months.\n* **Seasonal variations:** There are significant fluctuations in sales during the summer months.\n* **Top-selling products:** The data can help identify the most popular products based on their sales performance.\n* **Customer demographics:** The data can reveal insights about the demographics of customers, such as age, location, and spending habits.\n\nBy visualizing these insights,\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What is data visualization?\n"},{"name":"stdout","text":"AI Assistant: Data Visualization is one of the most important branches of data science. It is one of the main tools used to analyze and study relationships between different variables. Data visualization (e.g., scatter plots, line graphs, bar plots, histograms, qqplots, smooth densities, boxplots, pair plots, heat maps, etc.) can be used for descriptive analytics. Data visualization is also used in machine learning for data preprocessing and analysis, feature selection, model building, model testing, and model evaluation. When preparing a data visualization, keep in mind that data visualization is more of an Art than Science. (related concept: What is data visualization?).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"AI Assistant: Goodbye! Have a great day.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SUMMARY\n\n**This code defines a function, generate_response, for an AI assistant that combines retrieval and generative approaches. It first attempts to find a highly similar match in the provided search results based on user input and a labeled 'concept.' If a high similarity is found, it directly uses the matched information. Otherwise, it constructs a prompt by combining a set of predefined few-shot prompts with the user input and utilizes a language model to generate a response, seamlessly integrating user input with contextual prompts. The interactive loop allows users to engage with the AI assistant, prompting it with queries, and receiving responses that dynamically blend retrieved information and generated content.**","metadata":{}},{"cell_type":"markdown","source":"### **Sole Author and contributor to this work: Cheran Ratnam - cheran.jacob@gmail.com | 2149912389 **","metadata":{}}]}